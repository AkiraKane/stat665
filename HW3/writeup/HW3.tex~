\documentclass[a4paper,10pt]{article}
\usepackage[utf8x]{inputenc}

%opening
\title{Homework 3, STAT365/665}
\author{Girish Sastry}

\begin{document}

\maketitle

\section{Classification of Zipcode Data}

We use k-NN with $k=1,7,15$. We carry out LDA using the built in package in R.
For QDA, the process is a bit more complicated: because some of the covariance
matrices are singular, we must do some sort of dimensionality reduction before
running QDA. We use PCA to do this. Note that the prediction errors are 
essentially the same in both the 50 and 100 component cases, so for computational
efficiency we use the first 50 principal components. For logistic regression 
we make use of the multinom package. For reduced rank LDA, we first project
our data onto a subspace of dimension 10. Essentially, we define
$\hat{Y} = X(X^TX)^{-1}X^TY = X\hat{B}$. Next we compute the eigen-decomposition
of $\hat{Y}Y$ with PCA and we crossvalidate on the training set. PCA allows us
to choose the number of components each iteration to measure the error. The 
optimal number of components will minimize the error, and is the set of
components that we will use for reduced rank LDA. The optimal number of
components is 9 from crossvalidation. We obtain the following test and training
errors for the various methods:

\begin{tabular}{l*{6}{c}r}
 Method		& Test Error & Training Error \\
 \hline
 1-NN	& $2.47\%$ & $0.00\%$ \\
 7-NN 	& $3.02\%$ & $0.58\%$ \\
 15-NN	& $3.85\%$ & $0.93\%$ \\
 LDA	& $6.38\%$ & $6.20\%$ \\
 QDA	& $6.52\%$ & $1.77\%$ \\
 Reduced Rank LDA	& $6.38\%$ & $6.20\%$ \\
 Logistic Regression	& $9.49\%$ & $0.00\%$ \\
\end{tabular}

We see that in general, nearest neighbor methods once again perform better
than the other methods. Just as was the case for linear regression, this is 
probably due to the nature of the problem, and perhaps due to our error metric
for cross validation. Zipcode classification is a discrete classification 
problem and thus we use misclassification error instead of mean squared error
(as in regression). Thus, we note that linear methods generally do not perform
as well as nearest neighbors for the zipcode problem. Also, according to 
\textit{The Elements of Statistical Learning} website, a 2.5\% ter


rather than mean squared error. We note that in general, linear methods do not perform as well
as nearest-neighbors for this classiﬁcation problem; given that the Hastie/Tibshirani/Friedman
website states that a 2.5 percent testing error is “excellent”, it is likely that the optimal method is
much more complicated than linear. There is also the Gaussian assumption inherent in all of these
methods (except nearest-neighbors); in the context of grayscale images this assumption may be a
strong one, and as such one that causes LDA, QDA, reduced-rank LDA, and logistic regression to
perform decently worse than nearest-neighbor methods. (We also note that as k grows,
k-nearest-neighbors converges to ordinary least squares; this is why 7-nearest neighbors
outperforms 15-nearest-neighbors in terms of both training and test error rate).
We also note that (as expected), the reduced-rank LDA and LDA methods give identical results to
working precision; although the reduced-rank LDA used 9 eigenvectors as the basis for the

Page 2

Aneesh Raghunandan
STAT 365b Homework 3
3/5/2010
low-dimension subspace and not all 10 eigenvectors (in which case the reduced-rank LDA and
regular LDA would be guaranteed to give identical results), the proportion of variance on the
tenth is small enough that it is more or less irrelevant. Of particular interest is the superiority of
QDA to LDA, logistic regression, and reduced-rank LDA; this is likely due to the aforementioned
Gaussian assumption. QDA relaxes the assumption of a common covariance matrix, and instead
allows for within-class variability. Given the nature of these images, this imposes fewer parameter
restrictions upon the model and seems more reasonable than assuming a single variance across all
groups.
\section{Classification of Phoneme Data}

\end{document}
